{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Course Plan 12/02/2019</span>\n",
    "## <span style=\"color:blue\">(Last Updated 12/02/2019)</span>\n",
    "\n",
    "## Updated schedule through the rest of the semester\n",
    "\n",
    "\n",
    "|  Wk   |  M    |  W     | Topic   | Notebooks | Due |\n",
    "| :---: | :---: | :----: | :------ | :----- | :---: |\n",
    "|  8  |  10/21  | 23  | **Numpy:** Data Abstraction, **Numpy:** Multi-dimensional arrays,  | Midterm, 03-01, 03-02 | 10/30 |\n",
    "|  9  |  28  | 30  | **Numpy:** Reading into multi-dimensional arrays, **Pandas:** Dataframes and reading into them;  Merging and matching Dataframes| 03-03, 03-04, 03-05 | 10/30 |\n",
    "|  10  |  11/4  | 6  | **Pandas:** , Series and Views; Wrap Up Unit 3| 03-06, 03-07 | 11/10 |\n",
    "|  11 |  &mdash; | 13   | Classification and Clustering, **Case Study:** Iris Data Set | 04-02, 04-03  | 11/17 |\n",
    "|   |    |    | Notebooks under development&dagger;  | <del>04-04, 04-06, 04-07</del>  |\n",
    "|  12 |  18  | 20  | _k_-means Clustering, **Case Study:** [World Happiness Report](https://worldhappiness.report/ed/2019/), Recommendations  | 04-04, 05-01, 04-05 | 11/24 | \n",
    "|  13 |  25   | &mdash;  | <span style=\"color:blue\"> **Case Study:** Movie Recommendations</span> | 05-02 | 12/01 | \n",
    "|  14 |  12/2 | 4 |  **Case Studies:** World Happiness Map using [Geopandas](http://geopandas.org/) &Dagger;, Twitter Stream Analysis | 05-03, 05-04 | 12/08 |\n",
    "|  16 |  | 12/13 | **(Take Home) Final Exam**  |\n",
    "\n",
    "&dagger; We will not be covering these notebooks this semester. Feel free to peruse them if interested.\n",
    "\n",
    "&Dagger; Installing Geopandas broke my Jupyter environment! We will not be covering Geopandas. Wednesday's class will be used for previewing the final exam. The final will pick from a dataset in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). If you wish to practice, pick one of those datasets. Many (most?) of them have a \"Papers That Cite This Data Set\" which could serve as example questions. However, some of those analyses can get complex &mdash; if it looks too complex for the final, it probably is!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Stream Analysis\n",
    "\n",
    "<img align=\"right\" style=\"padding-left:10px; height: 50%; width: 50%\" src=\"figures/twitterbirds-980x589.png\" ></a>\n",
    "\n",
    "This case study seeks to read incoming tweets and process them.\n",
    "\n",
    "## Tweepy\n",
    "\n",
    "Modern Data sources typically deliver their data over an API. In this exercise, we will first acquire some tweets and work with them in various ways.\n",
    "\n",
    "We'll be working with [`tweepy`](https://github.com/tweepy/tweepy), an API for acquiring Data from Twitter.The best place to start is the [documentation](http://docs.tweepy.org/en/latest/). Take a look at the Getting Started page.\n",
    "\n",
    "To begin, uncomment the next cell and run it to install `tweepy` using pip. You need only do it once (but doing it more than once won't hurt anything). Run the next cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /opt/tljh/user/lib/python3.6/site-packages (3.8.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/tljh/user/lib/python3.6/site-packages (from tweepy) (1.12.0)\r\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /opt/tljh/user/lib/python3.6/site-packages (from tweepy) (1.6.8)\r\n",
      "Requirement already satisfied: requests>=2.11.1 in /opt/tljh/user/lib/python3.6/site-packages (from tweepy) (2.12.4)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/tljh/user/lib/python3.6/site-packages (from tweepy) (1.3.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/tljh/user/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to obtain Twitter credentials for being able to acccess the API. Visit the [Twitter Developer Site for Apps](https://developer.twitter.com/en/apps)</a> to create a twitter \"application,\" give it a name. Once created, go to the keys and tokens tab and copy-paste the Consumer API keys as well as the Access token & access token secret into the cell below. \n",
    "\n",
    "**Careful!** `consumer_secret` and `access_token_secret` should be protected like passwords because they can be used by the API to _send out tweets or direct messages_ on your behalf. Immediately invalidate the token if you accidentally expose it &mdash; regenerating a new set of token values on the keys and tokens page will invalidate the old one. \n",
    "\n",
    "To protect Twitter credentials from being made public I suggest that you keep the lines of the next cell in a separate file on your laptop. Running the next cell will get the variable `auth` into the IPython environment and you may immediately delete the cell after that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and Exceptions\n",
    "\n",
    "When dealing with live feeds, the program needs to be able to react to adverse conditions. A [user-defined exception](https://docs.python.org/3.8/tutorial/errors.html#user-defined-exceptions) should be defined for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamAnalysisError(Exception):\n",
    "    \"\"\"Base class for exceptions in this module.\"\"\"\n",
    "    pass\n",
    "\n",
    "class TweepyError(StreamAnalysisError):\n",
    "    \"\"\"Exception raised in Tweepy.\n",
    "\n",
    "    Attributes:\n",
    "        expression -- input expression in which the error occurred\n",
    "        message -- explanation of the error\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, expression, message = ''):\n",
    "        self.expression = expression\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, string, re\n",
    "from datetime import datetime\n",
    "import tweepy\n",
    "\n",
    "def analyze_tweet(stripped_text):\n",
    "    return []\n",
    "\n",
    "def show_tweet(tweet):\n",
    "    try:\n",
    "        return ' '.join([tweet.created_at.strftime(\"%m/%d/%Y %H:%M:%S\"), \n",
    "                         tweet.user.screen_name, \n",
    "                         tweet.text])\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "    \n",
    "class TwitterStream:\n",
    "    def __init__(self, auth):\n",
    "        self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "        # We will use this table to process the tweet text.\n",
    "        # Ref: https://stackoverflow.com/a/34294398/653651 \n",
    "        #      Remove all punctuation marks from a text file\n",
    "        #      Also remove ellipses but don't remove '@'\n",
    "        self.table = str.maketrans(dict.fromkeys((string.punctuation + \"…\").replace('@','')))\n",
    "    \n",
    "    def test(self):\n",
    "        public_tweets = self.api.home_timeline()\n",
    "        for tweet in public_tweets[:1]:\n",
    "            x = show_tweet(tweet)\n",
    "            assert (x) # Unable to get tweets, reason unknown\n",
    "        now = int(datetime.timestamp(datetime.now()))\n",
    "        # when the rate limit will reset\n",
    "        self.tweet_rate_reset = int(self.api.last_response.headers['x-rate-limit-reset'])\n",
    "        self.tweets_remaining = int(self.api.last_response.headers['x-rate-limit-remaining'])\n",
    "        self.tweet_rate_limit = int(self.api.last_response.headers['x-rate-limit-limit'])\n",
    "        \n",
    "    def rate_limit_delay(self):\n",
    "        '''\n",
    "        Status of the API vis-a-vis rate limitation\n",
    "        Based on https://developer.twitter.com/en/docs/basics/rate-limiting\n",
    "        '''\n",
    "        now = int(datetime.timestamp(datetime.now()))\n",
    "\n",
    "        self.tweet_rate_reset = int(self.api.last_response.headers['x-rate-limit-reset'])\n",
    "        self.tweets_remaining = int(self.api.last_response.headers['x-rate-limit-remaining'])\n",
    "        self.tweet_rate_limit = int(self.api.last_response.headers['x-rate-limit-limit'])\n",
    "\n",
    "        diff = now - self.tweet_rate_reset\n",
    "        trigger_after = 1 + reset\n",
    "        if (remaining <= 1):\n",
    "            if now < trigger_after:\n",
    "                print ('pause till', datetime.fromtimestamp(trigger_after).strftime(\"%H:%M:%S\"))\n",
    "                time.sleep (trigger_after - now)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        print ('ready')\n",
    "        return\n",
    "    \n",
    "    def on_rate_limit(self, cursor):\n",
    "        while True:\n",
    "            try:\n",
    "                yield cursor.next()\n",
    "            except tweepy.RateLimitError:\n",
    "                print ('tweepy.RateLimitError')\n",
    "                self.rate_limit_delay()\n",
    "            except StopIteration:\n",
    "                raise TweepyError('Ending tweets')\n",
    "\n",
    "    def get_more_tweets(self, user_id, count = 10, show = False):\n",
    "        # Reference: http://docs.tweepy.org/en/v3.8.0/code_snippet.html#handling-the-rate-limit-using-cursors\n",
    "        tweets_cursor = tweepy.Cursor(self.api.user_timeline, id=user_id) \n",
    "        try:\n",
    "            for tweet in self.on_rate_limit(tweets_cursor.items(count)):\n",
    "                tweet_text = tweet.text\n",
    "                text_no_url = re.sub(r'\\shttps?:\\/\\/.*[\\r\\n]*', '', tweet_text, flags=re.MULTILINE)\n",
    "                stripped_text = text_no_url.translate(self.table)\n",
    "                if show:\n",
    "                    print (tweet_text)\n",
    "                categories = analyze_tweet(stripped_text)\n",
    "                if show and categories:\n",
    "                    print ('-------------tweet-------------\\n' + tweet_text)\n",
    "                    print (categories)            \n",
    "\n",
    "        except TweepyError as tweepy_error:\n",
    "            now = datetime.now()\n",
    "            print (tweepy_error.expression, 'at', now.strftime(\"%m/%d/%Y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment as appropriate\n",
    "\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "# auth = tweepy.OAuthHandler(_creds[\"consumer_key\"], _creds[\"consumer_secret\"])\n",
    "# auth.set_access_token(_creds[\"access_token\"], _creds[\"access_token_secret\"])\n",
    "twitter_stream = TwitterStream(auth)\n",
    "twitter_stream.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Tweets\n",
    "\n",
    "In the next cell, if everything is working, we should see some tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“You can physically escape, but your heart is always trapped.” A secret pipeline has helped hundreds of Hong Kong p… https://t.co/m7ihS5xcq8\n",
      "Gal Gadot is back in the sequel, debuting Wonder Woman’s Golden Eagle Armor. And somehow, so is Steve Trevor, who s… https://t.co/mihaktTUKs\n",
      "“Feminist media has been especially hard hit by the financial turbulence in the news industry,” writes Emma Goldberg https://t.co/LCjjj7ZKxX\n",
      "2 prisoners became lovers in Auschwitz. When they reunited 72 years later, he had one question to ask her.… https://t.co/C63d5eNTd8\n",
      "The loss of the world’s primary trade referee could turn the typically deliberate process of resolving internationa… https://t.co/p7nMfoULMY\n",
      "Here’s what happened this week in the 2020 U.S. presidential race https://t.co/b9N7j5XgvA\n",
      "A 5-year-old, wearing just socks and light clothing, carried an 18-month-old through subzero temperatures in Alaska… https://t.co/e3P5cetGgn\n",
      "Sweet, stunning and unexpected: @NYTFood took over 8 pages of the newspaper to show you our holiday cookie spread.… https://t.co/lNPu2f4K0M\n",
      "“The lock screen on my phone says, ‘No.’ That’s my promise to myself to rest,” says Cheslie Kryst, who was crowned … https://t.co/8YgzrVVNch\n",
      "The Weeknd, Grimes, Kali Uchis and more: Here’s your weekend playlist https://t.co/xjaTB9SFhI\n",
      "Ending tweets at 12/08/2019 14:59:45\n"
     ]
    }
   ],
   "source": [
    "twitter_stream.get_more_tweets('nytimes', count = 10, show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moral Foundations Theory\n",
    "\n",
    "<img align=\"right\" style=\"padding-left:10px; height: 55%; width: 55%\" src=\"figures/political-camps-moral-foundations.png\" ></a>\n",
    "\n",
    "The second part of this case study is Moral Foundations Theory. Google the phrase or visit [MoralFoundations.org](https://moralfoundations.org/) to learn more about the theory (or watch [Jonathan Haidt's Ted Talk](https://www.ted.com/talks/jonathan_haidt_the_moral_roots_of_liberals_and_conservatives/)).\n",
    "\n",
    "For the purpose of this exercise, you needn't learn much about the theory &mdash; we will be using the Moral Foundations team's list of words that connote different dimensions of morality. \n",
    "\n",
    "* **Care** connotes safety, peace, compassion, etc.\n",
    "* **Harm**, Care's opposite, connotes war, fight, hurt, kill, suffer, etc.\n",
    "\n",
    "In the coding, the above two moral opposites are called 'HarmVirtue' and 'HarmVice' respectively. Similarly for other moral dimensions. The categories and the words that belong to each are available on-line in a [Moral Foundations Dictionary](https://moralfoundations.org/wp-content/uploads/files/downloads/moral%20foundations%20dictionary.dic). The dictionary words often include &ast;s, for example `peace`&ast;, which could be peace, peaceful, peacefully, even peacenik!\n",
    "\n",
    "To retrieve the Moral Foundations Dictionary we shall use the `requests` library, documentation available [here](https://requests.readthedocs.io/en/master/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://moralfoundations.org/wp-content/uploads/files/downloads/moral%20foundations%20dictionary.dic\"\n",
    "r = requests.get(url)\n",
    "content = r.text \n",
    "lines = content.split('\\n')\n",
    "groups = {}\n",
    "codes = {}\n",
    "reading_groups = False\n",
    "\n",
    "for line in lines:\n",
    "    line_ = ' '.join(line.split())\n",
    "    if not line_: continue\n",
    "    if line_.startswith('%'):\n",
    "        reading_groups = not reading_groups\n",
    "    else:\n",
    "        line__ = line.split()\n",
    "        if reading_groups:\n",
    "            groups[line__[0]] = line__[1]\n",
    "        else:\n",
    "            codes[line__[0]] = ','.join(line__[1:])\n",
    "# print (groups)\n",
    "\n",
    "# Prints:\n",
    "# {'01': 'HarmVirtue', '02': 'HarmVice', '03': 'FairnessVirtue', '04': 'FairnessVice', \n",
    "#  '05': 'IngroupVirtue', '06': 'IngroupVice', '07': 'AuthorityVirtue', '08': 'AuthorityVice', \n",
    "#  '09': 'PurityVirtue', '10': 'PurityVice', '11': 'MoralityGeneral'}\n",
    "\n",
    "# print (codes)\n",
    "\n",
    "# Partial print of codes:\n",
    "# {'safe*': '01', 'peace*': '01', 'compassion*': '01', 'empath*': '01', 'sympath*': '01', 'care': '01', \n",
    "#  …\n",
    "#  'deserter*': '06,08', \n",
    "#  …\n",
    "#  'transgress*': '11'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Design\n",
    "\n",
    "The obvious design is to create a dictionary that maps each word &rArr; category. Then we could take each word in an incoming tweet and categorize it. _But the &ast;s throw a wrench into this idea!_ How would it categorize \"He came peacefully\" when a dictionary whose key was peace&ast; wouldn't match the word \"peacefully!\"\n",
    "\n",
    "And if a dictionary won't solve the problem, won't looking up each tweet word without the benefit of a dictionary be too slow?\n",
    "\n",
    "Our design will be based on a combination of strategies: using a dictionary with 3-letter keys, and having each key map to a (relatively short) list that could be scanned quickly. For example, one of the longest lists will be \"dis\": [discriminat&ast;, disproportion&ast;, dishonest, dissociate, disloyal&ast;, dissent&ast;, disrespect&ast;, disobe&ast;, dissident, disgust&ast;, disease&ast;]. A particular Python class for this is a `defaultdict`, shown [here by example](https://docs.python.org/3.8/library/collections.html#defaultdict-examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'saf': [{'safe*': '01'}], 'pea': [{'peace*': '01'}], 'com': [{'compassion*': '01'}, {'communal': '05'}, {'commune*': '05'}, {'communit*': '05'}, {'communis*': '05'}, {'comrad*': '05'}, {'complian*': '07'}, {'command': '07'}, {'comply': '07'}, {'commendable': '11'}], 'emp': [{'empath*': '01'}], 'sym': [{'sympath*': '01'}], 'car': [{'care': '01'}, {'caring': '01'}], 'pro': [{'protect*': '01'}, {'protest': '08'}, {'profan*': '10'}, {'promiscu*': '10'}, {'prostitut*': '10'}, {'profligate': '10'}, {'proper': '11'}], 'shi': [{'shield': '01'}], 'she': [{'shelter': '01'}], 'ami': [{'amity': '01'}], 'sec': [{'secur*': '01'}], 'ben': [{'benefit*': '01'}], 'def': [{'defen*': '01'}, {'defere*': '07'}, {'defer': '07'}, {'defian*': '08'}, {'defy*': '08'}, {'defector': '08'}, {'defile*': '10'}], 'gua': [{'guard*': '01'}], 'pre': [{'preserve': '01,07,09'}, {'prejud*': '04'}, {'preference': '04'}], 'har': [{'harm*': '02'}], 'suf': [{'suffer*': '02'}], 'war': [{'war': '02'}, {'wars': '02'}, {'warl*': '02'}, {'warring': '02'}], 'fig': [{'fight*': '02'}], 'vio': [{'violen*': '02'}], 'hur': [{'hurt*': '02'}], 'kil': [{'kill': '02'}, {'kills': '02'}, {'killer*': '02'}, {'killed': '02'}, {'killing': '02'}], 'end': [{'endanger*': '02'}], 'cru': [{'cruel*': '02'}, {'crush*': '02'}], 'bru': [{'brutal*': '02'}], 'abu': [{'abuse*': '02'}], 'dam': [{'damag*': '02'}], 'rui': [{'ruin*': '02,10'}], 'rav': [{'ravage': '02'}], 'det': [{'detriment*': '02'}], 'att': [{'attack*': '02'}], 'ann': [{'annihilate*': '02'}], 'des': [{'destroy': '02'}, {'deserted': '06,08'}, {'deserter*': '06,08'}, {'deserting': '06,08'}, {'desecrat*': '10'}], 'sto': [{'stomp': '02'}], 'aba': [{'abandon*': '02,06'}], 'spu': [{'spurn': '02'}], 'imp': [{'impair': '02'}, {'impartial*': '03'}, {'imposter': '06'}, {'impiety': '10'}, {'impious': '10'}], 'exp': [{'exploit': '02,10'}, {'exploits': '02,10'}, {'exploited': '02,10'}, {'exploiting': '02,10'}, {'exploitat*': '10'}], 'wou': [{'wound*': '02'}], 'fai': [{'fair': '03'}, {'fairly': '03'}, {'fairness': '03'}, {'fair-*': '03'}, {'fairmind*': '03'}, {'fairplay': '03'}], 'equ': [{'equal*': '03'}, {'equity': '03'}, {'equivalent': '03'}, {'equable': '03'}], 'jus': [{'justice': '03'}, {'justness': '03'}, {'justifi*': '03'}], 'rec': [{'reciproc*': '03'}], 'ega': [{'egalitar*': '03'}], 'rig': [{'rights': '03'}, {'righteous*': '11'}], 'eve': [{'evenness': '03'}], 'unb': [{'unbias*': '03'}], 'tol': [{'tolerant': '03'}], 'bal': [{'balance*': '03'}], 'hom': [{'homologous': '03'}, {'homeland*': '05'}], 'unp': [{'unprejudice*': '03'}], 'rea': [{'reasonable': '03'}], 'con': [{'constant': '03'}, {'control': '07'}, {'contagio*': '10'}], 'hon': [{'honest*': '03,11'}, {'honor*': '07'}], 'unf': [{'unfair*': '04'}, {'unfaithful': '08'}], 'une': [{'unequal*': '04'}], 'bia': [{'bias*': '04'}], 'unj': [{'unjust*': '04'}], 'inj': [{'injust*': '04'}], 'big': [{'bigot*': '04'}], 'dis': [{'discriminat*': '04'}, {'disproportion*': '04'}, {'dishonest': '04'}, {'dissociate': '04'}, {'disloyal*': '06,08'}, {'dissent*': '08'}, {'disrespect*': '08'}, {'disobe*': '08'}, {'dissident': '08'}, {'disgust*': '10'}, {'disease*': '10'}], 'ine': [{'inequitable': '04'}], 'uns': [{'unscrupulous': '04'}], 'fav': [{'favoritism': '04'}], 'seg': [{'segregat*': '04,05'}], 'exc': [{'exclusion': '04'}, {'exclud*': '04'}], 'tog': [{'together': '05'}], 'nat': [{'nation*': '05'}], 'fam': [{'family': '05'}, {'families': '05'}, {'familial': '05'}], 'gro': [{'group': '05'}, {'gross': '10'}], 'loy': [{'loyal*': '05,07'}], 'pat': [{'patriot*': '05'}], 'cad': [{'cadre': '05'}], 'col': [{'collectiv*': '05'}], 'joi': [{'joint': '05'}], 'uni': [{'unison': '05'}, {'unite*': '05'}], 'fel': [{'fellow*': '05'}], 'gui': [{'guild': '05'}], 'sol': [{'solidarity': '05'}], 'dev': [{'devot*': '05'}], 'mem': [{'member': '05'}], 'cli': [{'cliqu*': '05'}], 'coh': [{'cohort': '05'}], 'all': [{'ally': '05'}, {'allegian*': '07'}], 'ins': [{'insider': '05'}, {'insubordinat*': '08'}, {'insurgent': '08'}], 'for': [{'foreign*': '06'}], 'ene': [{'enem*': '06'}], 'bet': [{'betray*': '06,08'}], 'tre': [{'treason*': '06,08'}, {'treacher*': '06,08'}], 'tra': [{'traitor*': '06,08'}, {'tradition*': '07'}, {'tramp': '10'}, {'trashy': '10'}, {'transgress*': '11'}], 'ind': [{'individual*': '06'}, {'indecen*': '10,11'}], 'apo': [{'apostasy': '06,08,10'}, {'apostate': '06,08,10'}], 'dec': [{'deceiv*': '06'}, {'decen*': '09,11'}], 'jil': [{'jilt*': '06'}], 'mis': [{'miscreant': '06'}], 'spy': [{'spy': '06'}], 'seq': [{'sequester': '06'}], 'ren': [{'renegade': '06'}], 'ter': [{'terroris*': '06'}], 'imm': [{'immigra*': '06'}, {'immaculate': '09'}, {'immoral*': '11'}], 'obe': [{'obey*': '07'}, {'obedien*': '07'}], 'dut': [{'duty': '07'}, {'duti*': '07'}], 'law': [{'law': '07'}, {'lawful*': '07,11'}, {'lawless*': '08'}], 'leg': [{'legal*': '07,11'}], 'res': [{'respect': '07'}, {'respectful*': '07'}, {'respected': '07'}, {'respects': '07'}], 'ord': [{'order*': '07'}], 'fat': [{'father*': '07'}], 'mot': [{'mother': '07'}, {'motherl*': '07'}, {'mothering': '07'}, {'mothers': '07'}], 'hie': [{'hierarch*': '07'}], 'aut': [{'authorit*': '07'}], 'per': [{'permit': '07'}, {'permission': '07'}, {'pervert': '10'}], 'sta': [{'status*': '07'}, {'stain*': '10'}], 'ran': [{'rank*': '07'}], 'lea': [{'leader*': '07'}], 'cla': [{'class': '07'}], 'bou': [{'bourgeoisie': '07'}], 'cas': [{'caste*': '07'}], 'pos': [{'position': '07'}], 'sup': [{'supremacy': '07'}], 'sub': [{'submi*': '07'}, {'subver*': '08'}], 'ser': [{'serve': '07'}], 'abi': [{'abide': '07'}], 'rev': [{'revere*': '07'}], 'ven': [{'venerat*': '07'}], 'reb': [{'rebel*': '08'}], 'sed': [{'sediti*': '08'}], 'agi': [{'agitat*': '08'}], 'ill': [{'illegal*': '08'}], 'mut': [{'mutinous': '08'}], 'ali': [{'alienate': '08'}], 'her': [{'heretic*': '08,10'}], 'non': [{'nonconformist': '08'}], 'opp': [{'oppose': '08'}], 'ref': [{'refuse': '08'}, {'refined': '09'}], 'den': [{'denounce': '08'}], 'rem': [{'remonstrate': '08'}], 'rio': [{'riot*': '08'}], 'obs': [{'obstruct': '08'}, {'obscen*': '10'}], 'pie': [{'piety': '09,11'}], 'pio': [{'pious': '09,11'}], 'pur': [{'purity': '09'}, {'pure*': '09'}], 'cle': [{'clean*': '09'}], 'ste': [{'steril*': '09'}], 'sac': [{'sacred*': '09'}], 'cha': [{'chast*': '09'}, {'character': '11'}], 'hol': [{'holy': '09'}, {'holiness': '09'}], 'sai': [{'saint*': '09'}], 'who': [{'wholesome*': '09,11'}, {'whore': '10'}], 'cel': [{'celiba*': '09'}], 'abs': [{'abstention': '09'}, {'abstinen*': '09'}, {'abstemiousness': '09'}], 'vir': [{'virgin': '09'}, {'virgins': '09'}, {'virginity': '09'}, {'virginal': '09'}, {'virtuous': '09'}], 'aus': [{'austerity': '09'}], 'int': [{'integrity': '09,11'}, {'intemperate': '10'}], 'mod': [{'modesty': '09'}], 'upr': [{'upright': '09,11'}], 'lim': [{'limpid': '09'}], 'una': [{'unadulterated': '09'}], 'mai': [{'maiden': '09'}], 'inn': [{'innocent': '09'}], 'pri': [{'pristine': '09'}, {'principle*': '11'}], 'chu': [{'church*': '09'}], 'dep': [{'deprav*': '10'}], 'unc': [{'unclean*': '10'}, {'unchaste': '10'}], 'sin': [{'sin': '10'}, {'sinful*': '10'}, {'sinner*': '10'}, {'sins': '10'}, {'sinned': '10'}, {'sinning': '10'}], 'slu': [{'slut*': '10'}], 'dir': [{'dirt*': '10'}], 'rep': [{'repuls*': '10'}], 'sic': [{'sick*': '10'}], 'lew': [{'lewd*': '10'}], 'adu': [{'adulter*': '10'}], 'deb': [{'debauche*': '10'}, {'debase*': '10'}], 'wan': [{'wanton': '10'}], 'fil': [{'filth*': '10'}], 'lax': [{'lax': '10'}], 'tai': [{'taint*': '10'}], 'tar': [{'tarnish*': '10'}], 'wic': [{'wicked*': '10,11'}], 'ble': [{'blemish': '10'}], 'wre': [{'wretched*': '10,11'}], 'mor': [{'moral*': '11'}], 'eth': [{'ethic*': '11'}], 'val': [{'value*': '11'}], 'ups': [{'upstanding': '11'}], 'goo': [{'good': '11'}, {'goodness': '11'}], 'bla': [{'blameless': '11'}], 'exe': [{'exemplary': '11'}], 'les': [{'lesson': '11'}], 'can': [{'canon': '11'}], 'doc': [{'doctrine': '11'}], 'nob': [{'noble': '11'}], 'wor': [{'worth*': '11'}], 'ide': [{'ideal*': '11'}], 'pra': [{'praiseworthy': '11'}], 'lau': [{'laudable': '11'}], 'cor': [{'correct': '11'}], 'wro': [{'wrong*': '11'}], 'evi': [{'evil': '11'}], 'bad': [{'bad': '11'}], 'off': [{'offend*': '11'}, {'offensive*': '11'}]})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "groups = {}\n",
    "codes = defaultdict(list)\n",
    "reading_groups = False\n",
    "\n",
    "for line in lines:\n",
    "    line_ = ' '.join(line.split())\n",
    "    if not line_: continue\n",
    "    if line_.startswith('%'):\n",
    "        reading_groups = not reading_groups\n",
    "    else:\n",
    "        line__ = line.split()\n",
    "        if reading_groups:\n",
    "            groups[line__[0]] = line__[1]\n",
    "        else:\n",
    "            '''\n",
    "            We have to iterate through the words in a tweet \n",
    "            To allow for fast lookups, we make a dictionary with 3-letter keys\n",
    "            Ref: https://docs.python.org/3.3/library/collections.html#defaultdict-examples\n",
    "            '''\n",
    "            codes[line__[0][0:3]].append({line__[0]:(','.join(line__[1:]))})\n",
    "\n",
    "print (codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function for finding categories of a word\n",
    "\n",
    "We want a function `find_word_categories(word)` such that the **assert** statements at the end of the next cell all pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02']\n",
      "['07']\n",
      "['10']\n",
      "['01', '07', '09']\n"
     ]
    }
   ],
   "source": [
    "def find_word_categories(word):\n",
    "    \n",
    "    head = word[0:3]\n",
    "    \n",
    "    for i in codes[head]:\n",
    "        \n",
    "        i=list(i.items())\n",
    "        k = i[0][0]\n",
    "        v = i[0][1]\n",
    "        \n",
    "        if k.endswith('*'):\n",
    "            if k[:-1] in word:\n",
    "                print(v.split(','))\n",
    "                return v.split(',')\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if k in word:\n",
    "                print(v.split(','))\n",
    "                return v.split(',')\n",
    "            else:\n",
    "                continue\n",
    "    return []\n",
    "\n",
    "assert (find_word_categories('hurt') == ['02'])\n",
    "assert (find_word_categories('tradition') == ['07'])\n",
    "assert (find_word_categories('disease') == ['10'])\n",
    "assert (find_word_categories('@nytnational') == [])\n",
    "assert (find_word_categories('preserve') == ['01', '07', '09'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new `analyze_tweet` function\n",
    "\n",
    "Now the `analyze_tweet` function from before can be replaced by the real thing! Changing the definition of a function during the session is a powerful technique, but can result in confusion &mdash; which version of the program is being called at any moment? \n",
    "\n",
    "**The last function definition that was executed by IPython wins!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['07']\n",
      "['05']\n"
     ]
    }
   ],
   "source": [
    "def analyze_tweet(tweet_text):\n",
    "    #print (tweet_text)\n",
    "    text_no_url = re.sub(r'\\shttps?:\\/\\/.*[\\r\\n]*', '', tweet_text, flags=re.MULTILINE)\n",
    "    stripped_text = text_no_url.translate(twitter_stream.table)\n",
    "    tw_words = stripped_text.lower().split(' ')\n",
    "    categories = []\n",
    "    for tw_word in tw_words:\n",
    "        cats = find_word_categories(tw_word)\n",
    "        for cat in cats:\n",
    "            categories.append({groups[cat]: tw_word})\n",
    "    return categories\n",
    "\n",
    "cats = analyze_tweet(\"A tradition can be many things — for some, it's food. For others, it's faith. For many, family. \")\n",
    "assert ({'AuthorityVirtue': 'tradition'} in cats)\n",
    "assert ({'IngroupVirtue': 'family'} in cats)\n",
    "assert (2 == len(cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally, categorized tweets!\n",
    "\n",
    "In the following, only the tweets that mentioned a moral foundation will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['07']\n",
      "['11']\n",
      "['10']\n",
      "['10']\n",
      "['02']\n",
      "['11']\n",
      "['07']\n",
      "['07']\n",
      "['02']\n",
      "['02']\n",
      "['05']\n",
      "['03']\n",
      "['07']\n",
      "['05']\n",
      "['11']\n",
      "['11']\n",
      "['02']\n",
      "['08']\n",
      "['02']\n",
      "['02']\n",
      "['11']\n",
      "['05']\n",
      "['07']\n",
      "['01']\n",
      "['03']\n",
      "['11']\n",
      "['07']\n",
      "['02']\n",
      "['01']\n",
      "['02']\n",
      "['07']\n",
      "['01']\n",
      "['02']\n",
      "['02']\n",
      "['02']\n",
      "['03']\n",
      "['05']\n",
      "['02']\n",
      "['05']\n",
      "['03']\n",
      "['08']\n",
      "['05']\n",
      "['05']\n",
      "['07']\n",
      "['02']\n",
      "['07']\n",
      "['01']\n",
      "['08']\n",
      "['03']\n",
      "['07']\n",
      "['05']\n",
      "['07']\n",
      "['10']\n",
      "['06']\n",
      "['01', '07', '09']\n",
      "['07']\n",
      "['07']\n",
      "['02']\n",
      "['11']\n",
      "['07']\n",
      "['03']\n",
      "['11']\n",
      "['07']\n",
      "['10']\n",
      "['07']\n",
      "['09']\n",
      "['07']\n",
      "['11']\n",
      "['01']\n",
      "['05']\n",
      "['03']\n",
      "['01']\n",
      "['03']\n",
      "['03']\n",
      "['07']\n",
      "['02']\n",
      "['05']\n",
      "['07']\n",
      "['02']\n",
      "['07']\n",
      "['09']\n",
      "['07']\n",
      "['07']\n",
      "['02']\n",
      "['02']\n",
      "['09']\n",
      "['07']\n",
      "['02']\n",
      "['08']\n",
      "['01']\n",
      "['01']\n",
      "['07']\n",
      "['10']\n",
      "['02']\n",
      "['05']\n",
      "['05']\n",
      "['01']\n",
      "['11']\n",
      "['08']\n",
      "['02']\n",
      "['07']\n",
      "['11']\n",
      "['11']\n",
      "['09']\n",
      "['03']\n",
      "['05']\n",
      "['02']\n",
      "['02']\n",
      "['05']\n",
      "['02']\n",
      "['07']\n",
      "['05']\n",
      "['01']\n",
      "['05']\n",
      "['02']\n",
      "['10']\n",
      "['01']\n",
      "['07', '11']\n",
      "['07']\n",
      "['07']\n",
      "['07']\n",
      "['05']\n",
      "['02']\n",
      "['07']\n",
      "['07']\n",
      "['11']\n",
      "['07']\n",
      "['07']\n",
      "['11']\n",
      "['07']\n",
      "['02']\n",
      "['07']\n",
      "['02']\n",
      "['07']\n",
      "['07']\n",
      "['11']\n",
      "['05']\n",
      "['11']\n",
      "['05']\n",
      "['07']\n",
      "['07']\n",
      "['02']\n",
      "['10']\n",
      "['02']\n",
      "['02']\n",
      "['05']\n",
      "['05']\n",
      "['05']\n",
      "['07']\n",
      "['02']\n",
      "['07']\n",
      "['05']\n",
      "['11']\n",
      "['07']\n",
      "['02']\n",
      "['03']\n",
      "['07']\n",
      "['05']\n",
      "['11']\n",
      "['05']\n",
      "['11']\n",
      "['05']\n",
      "['07']\n",
      "['07']\n",
      "['05']\n",
      "['10']\n",
      "['02']\n",
      "['06']\n",
      "['02']\n",
      "['07']\n",
      "['07']\n",
      "['07']\n",
      "['06']\n",
      "['07', '11']\n",
      "['07']\n",
      "['07', '11']\n",
      "['11']\n",
      "['07', '11']\n",
      "['07']\n",
      "['07']\n",
      "['02']\n",
      "['07']\n",
      "['09']\n",
      "['02']\n",
      "['05']\n",
      "['11']\n",
      "['07']\n",
      "['07']\n",
      "['05']\n",
      "['07']\n",
      "['07']\n",
      "['07']\n",
      "['03']\n",
      "['01']\n",
      "['01']\n",
      "['01']\n",
      "['07']\n",
      "['05']\n",
      "['02']\n",
      "['11']\n",
      "['10']\n",
      "['02']\n",
      "['07']\n",
      "Ending tweets at 12/08/2019 16:07:33\n"
     ]
    }
   ],
   "source": [
    "twitter_stream.get_more_tweets('nytimes', count = 500, show = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Question:\n",
    "\n",
    "In <em>your opinion</em>, how well does the categorization track your sense of the moral values expressed in the tweets? Do you see any blatant biases?\n",
    "\n",
    "_There is no right or wrong answer to this question. Your response will be judged by how well-reasoned it is._\n",
    "\n",
    "\n",
    "My answer: this method works, but not accurately works. Since the expression of meaning usually depends on context, using method with single world analysis can not be accurate. For instance, \"he is good\" and \"he is not good\" both contains world \"good\" but expresses an opposite meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Ups?\n",
    "\n",
    "It would be cool to plot a graph of the (Virtue - Vice) values of the five foundations expressed in these tweets. \n",
    "\n",
    "The New York Times is reputed to be a center-left news source. How do the foundations expressed by this source compared with the foundations expressed by other sources? \n",
    "\n",
    "In case you were wondering, there are [some serious objections](https://behavioralscientist.org/whats-wrong-with-moral-foundations-theory-and-how-to-get-moral-psychology-right/) to Moral Foundation Theory; which you may or may not find persuasive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
